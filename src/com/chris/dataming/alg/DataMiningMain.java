package com.chris.dataming.alg;

import java.io.FileNotFoundException;
import java.io.IOException;
import java.net.URI;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.FileUtil;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Counters;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;


import  com.chris.dataming.apriori.*;


public class DataMiningMain extends Configured implements Tool {

	/**
	 * @param args
	 */
	public static void main(String[] args) {
		String[] args1 = { "hdfs://192.168.50.59:9000/cdr/20160601/*",
				"hdfs://192.168.50.59:9000/items/"

		};
		//test unit on book
		String[] args0 = { "hdfs://192.168.50.59:9000/cdr/20160603/*",
				"hdfs://192.168.50.59:9000/tmp/"

		};
		String[] args4 = { "hdfs://192.168.50.59:9000/cdr/20160602/*",
				"hdfs://192.168.50.59:9000/items/"

		};
		String[] args2 = { "hdfs://localhost:9000/cdr/*",
				"hdfs://localhost:9000/cdr/"


		};
		int ec = -1;
		try {
			ec = ToolRunner.run(new Configuration(), new DataMiningMain(),
					args);
		} catch (Exception e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
		System.exit(ec);
	}

	@Override
	public int run(String[] args) throws Exception {
		// TODO Auto-generated method stub
		String inputDataPath = args[0];
		//hdfs://localhost:9000/cdr/output1/ 
		String outputFirstDataPath = args[1]+"output1/";
		String outputPath = args[1];
		long minsup =Long.parseLong(args[2]);
		
		Configuration conf = new Configuration();// 读取配置文件
		//conf.setBoolean("fs.hdfs.impl.disable.cache", true);
		
		long trans_num = 100000;//GetTransNum(inputDataPath, outputFirstDataPath, conf);// job1,get numbers of all items
																			
		

		 //FileSystem fs = FileSystem.get(new URI(args[1]), conf);
		// long trans_num = Tools.readHdfsFile(outputFirstDataPath,fs,conf);

		System.out.println("trans_num->" + trans_num);
		//long minsup = args[2];//(long) (trans_num * 0.8);// minimus support base on 2/8 rule
												
		long k = 1;
		// 获得一项频繁集集合，并将结果放入路径outputpath
		GetFirstFreq(inputDataPath, outputFirstDataPath, k, trans_num, minsup,
				conf);
		k++;
		
		String kOutputPath = outputPath +"output"+k+"/";
		String kInputPath = outputFirstDataPath;
		System.out.println("inputDataPath"+inputDataPath+"\ninputPath:"+kInputPath+"\noutputPath:"+kOutputPath);
		
		 while(true){
			 //从outputPath获得 Lk，并将生成的 Ｌ(k+1)输出到新
			 //的路径k_outputPath 中
			 //String k_outputpath =
			 GetKFreq(inputDataPath,kInputPath,kOutputPath,k,minsup,conf);
			 if(isPathNull(kOutputPath,conf))
				 {break;}
			 
				 
			 System.out.println("k"+k);
			 k++;
			 kInputPath = kOutputPath;
			 kOutputPath = outputPath +"output"+k+"/";
			 System.out.println("inputPath:"+kInputPath+";outputPath:"+kOutputPath);
			 
			 if (k==4)break;//test. only run 3times
 		 }

		return 0;
	}
	
	boolean isPathNull(String path , Configuration conf) 
			throws IOException{
		Path p = new Path(path);
		FileSystem hdfs = p.getFileSystem(conf);

		if (!hdfs.isDirectory(p)) {
			return true;
		}else{
			  
		       
		        FileStatus status[] = hdfs.globStatus(p);  
		        if (status==null || status.length==0) {  
		        return true;
		          
		        }  
		        for(int i=0; i<status.length; i++) {  
		          long totalSize = hdfs.getContentSummary(status[i].getPath()).getLength();  
		          String pathStr = status[i].getPath().toString();  
		          System.out.println(("".equals(pathStr)?".":pathStr) + "\t" + totalSize); 
		          if (0 == totalSize)
		        	  return true;
		        }  
		}
		return false;
	}
	/**
	 * get k frequency of all records
	 * @param inputDataPath is all primitive records
	 * @param inputPath is k elements 
	 * @param outputPath is k+1 elements to be generated by k
	 * @throws IOException 
	 * @throws InterruptedException 
	 * @throws ClassNotFoundException 
	 * */
	long GetKFreq(String inputDataPath,String inputPath,String outputPath,long k,
			long minsup,Configuration conf) throws IOException, ClassNotFoundException, InterruptedException{
		Path myDataPath = new Path(inputDataPath);
		Path myInputPath = new Path(inputPath);
		Path myOutputPath = new Path(outputPath);
		
		conf.set("inputPathofItems", inputPath);
		conf.setLong("kvalue",k);
		//conf.setStrings(name, values)
		deletePath(myOutputPath, conf);

		@SuppressWarnings("deprecation")
		Job job = new Job(conf, "K_Frequence");// 新建一个任务
		job.setJarByClass(DataMiningMain.class);// 主类
		job.setMapperClass(AprioriKthFreqMapper.class);// Mapper
		job.setCombinerClass(KthFreqCombiner.class);// conbiner
		job.setReducerClass(AprioriKthFreqReduce.class);// Reducer

		job.setPartitionerClass(AprioriPartitioner.class);// 设置Partitioner类

		job.setNumReduceTasks(3);// reduce个数设置为3

		job.setMapOutputKeyClass(Text.class);// map 输出key类型
		job.setMapOutputValueClass(Text.class);// map 输出value类型

		job.setOutputKeyClass(Text.class);// 输出结果 key类型
		job.setOutputValueClass(Text.class);// 输出结果 value 类型

		FileInputFormat.addInputPath(job, myDataPath);// 输入路径
		FileOutputFormat.setOutputPath(job, myOutputPath);// 输出路径

		boolean submitJob = job.waitForCompletion(true);// 提交任务
		if (submitJob) {
			System.out.println("submitKthFrequencejob success!!!!!");
		} else {
			System.out.println("Kth Frequency job failed!!!!!");
		}	
		
	return 0l;
	}
/**
 * get One frequency of all records
 * */
	long GetFirstFreq(String inputDataPath, String outDataPath, long k,
			long trans_num, long minsup, Configuration conf)
			throws IOException, ClassNotFoundException, InterruptedException {

		Path myInputPath = new Path(inputDataPath);
		Path myOutputPath = new Path(outDataPath);
		conf.setLong("minsupport", minsup);
		deletePath(myOutputPath, conf);

		@SuppressWarnings("deprecation")
		Job job = new Job(conf, "firstFrequence");// 新建一个任务
		job.setJarByClass(DataMiningMain.class);// 主类
		job.setMapperClass(AprioriFirstFrequentMapper.class);// Mapper
		job.setCombinerClass(FirstCombiner.class);// conbiner
		job.setReducerClass(AprioriFirstFrequentReduce.class);// Reducer

		job.setPartitionerClass(AprioriPartitioner.class);// 设置Partitioner类

		job.setNumReduceTasks(3);// reduce个数设置为3

		job.setMapOutputKeyClass(Text.class);// map 输出key类型
		job.setMapOutputValueClass(Text.class);// map 输出value类型

		job.setOutputKeyClass(Text.class);// 输出结果 key类型
		job.setOutputValueClass(Text.class);// 输出结果 value 类型

		FileInputFormat.addInputPath(job, myInputPath);// 输入路径
		FileOutputFormat.setOutputPath(job, myOutputPath);// 输出路径

		boolean submitJob = job.waitForCompletion(true);// 提交任务
		if (submitJob) {
			System.out.println("submitFirstFrequencejob success!!!!!");
		} else {
			System.out.println("Frequencyfailed!!!!!");
		}

		return 0l;
	}

	/**
	 * get items numbers of all records
	 * */

	long GetTransNum(String inputDataPath, String outDataPath,
			Configuration conf) throws IOException, ClassNotFoundException,
			InterruptedException {
		Path myInputPath = new Path(inputDataPath);
		// Path myOutputPath = new Path("hdfs://192.168.50.60:9000/cdr/items/");
		Path myOutputPath = new Path(outDataPath);

		deletePath(myOutputPath, conf);

		@SuppressWarnings("deprecation")
		Job job = new Job(conf, "itemsNums");// 新建一个任务
		
		job.setJarByClass(DataMiningMain.class);// 主类
		job.setMapperClass(AprioriItemsMapper.class);// Mapper
		// job.setReducerClass(AprioriItemsReduce.class);//Reducer

		// job.setPartitionerClass(AprioriPartitioner.class);//设置Partitioner类
		job.setNumReduceTasks(0);// reduce个数设置为0

		job.setMapOutputKeyClass(Text.class);// map 输出key类型
		job.setMapOutputValueClass(LongWritable.class);// map 输出value类型

		job.setOutputKeyClass(Text.class);// 输出结果 key类型
		job.setOutputValueClass(Text.class);// 输出结果 value 类型

		FileInputFormat.addInputPath(job, myInputPath);// 输入路径
		FileOutputFormat.setOutputPath(job, myOutputPath);// 输出路径

		boolean submitJob = job.waitForCompletion(true);// 提交任务
		if (submitJob) {
			System.out.println("submitjob success!!!!!");
		} else {
			System.out.println("failed!!!!!");
		}
		long linesValue = job.getCounters()
				.findCounter(Tools.FileRecorder.TotalRecorder).getValue();
		System.out.println("TotalRecorder" + linesValue);
		return linesValue;
	}

	/**
	 * delete files in Path
	 * */
	int deletePath(Path p, Configuration c) throws IOException {
		FileSystem hdfs = p.getFileSystem(c);

		if (hdfs.isDirectory(p)) {
			hdfs.delete(p, true);
		}

		return 0;
	}
}
